{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Async Task Pipeline","text":"<p>A high-performance Python framework for processing streaming data through CPU-intensive tasks using async I/O and thread-based processing.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Async Task Pipeline framework provides a flexible and efficient way to build data processing pipelines that can handle streaming data with CPU-intensive tasks. It combines the benefits of async I/O for handling input/output operations with thread-based processing for CPU-bound computations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Async I/O Integration: Seamlessly process async iterators and generators</li> <li>Thread-based Processing: CPU-intensive tasks run in separate threads</li> <li>Performance Monitoring: Built-in timing and latency analysis</li> <li>Pipeline Composition: Chain multiple processing stages together</li> <li>Error Handling: Robust error handling and logging</li> <li>Memory Efficient: Queue-based processing with configurable limits</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import asyncio\nfrom async_task_pipeline import AsyncTaskPipeline\n\n# Create a pipeline\npipeline = AsyncTaskPipeline(max_queue_size=100, enable_timing=True)\n\n# Add processing stages\npipeline.add_stage(\"process\", lambda x: x * 2)\npipeline.add_stage(\"filter\", lambda x: x if x &gt; 10 else None)\n\n# Start the pipeline\nawait pipeline.start()\n\n# Process data\nasync def data_generator():\n    for i in range(20):\n        yield i\n        await asyncio.sleep(0.1)\n\n# Process input stream and collect results\nawait pipeline.process_input_stream(data_generator())\n\nresults = []\nasync for result in pipeline.generate_output_stream():\n    results.append(result)\n\n# Get performance statistics\nstats = pipeline.get_latency_summary()\nprint(f\"Processed {stats['total_items']} items\")\nprint(f\"Average latency: {stats['avg_total_latency']:.3f}s\")\n\n# Stop the pipeline\nawait pipeline.stop()\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework is built around three main components:</p> <ol> <li>AsyncTaskPipeline: Main orchestrator that manages stages and data flow</li> <li>PipelineStage: Individual processing stages that run in separate threads</li> <li>PipelineItem: Data containers that track timing and flow through the pipeline</li> </ol>"},{"location":"#pipeline-workflow","title":"Pipeline Workflow","text":"<p>The following diagram illustrates how data flows through the pipeline with parallel processing across multiple stages:</p> <pre><code>sequenceDiagram\n    participant Input as Async Input Stream\n    participant Main as Main Thread(Asyncio Event Loop)\n    participant Q1 as Input Queue\n    participant T1 as Thread 1(Stage 1: Validate)\n    participant Q2 as Queue 1\n    participant T2 as Thread 2(Stage 2: Transform)\n    participant Q3 as Queue 2\n    participant T3 as Thread 3(Stage 3: Serialize)\n    participant Q4 as Output Queue\n    participant Output as Async Output Stream\n\n    Note over Main: Pipeline Parallelism - Multiple items processed simultaneously\n\n    Input-&gt;&gt;Main: yield Item A\n    Main-&gt;&gt;Q1: put Item A\n    Q1-&gt;&gt;T1: get Item A\n\n    Input-&gt;&gt;Main: yield Item B\n    Main-&gt;&gt;Q1: put Item B\n    Q1-&gt;&gt;T1: get Item B\n\n    par Item A flows through pipeline\n        T1-&gt;&gt;Q2: put processed Item A\n        Q2-&gt;&gt;T2: get Item A\n        T2-&gt;&gt;Q3: put processed Item A\n        Q3-&gt;&gt;T3: get Item A\n        T3-&gt;&gt;Q4: put processed Item A\n    and Item B follows behind\n        T1-&gt;&gt;Q2: put processed Item B\n        Q2-&gt;&gt;T2: get Item B\n        T2-&gt;&gt;Q3: put processed Item B\n    and Item C enters pipeline\n        Input-&gt;&gt;Main: yield Item C\n        Main-&gt;&gt;Q1: put Item C\n        Q1-&gt;&gt;T1: get Item C\n        T1-&gt;&gt;Q2: put processed Item C\n    end\n\n    Q4-&gt;&gt;Main: get Item A\n    Main-&gt;&gt;Output: yield Item A\n\n    Q4-&gt;&gt;Main: get Item B\n    Main-&gt;&gt;Output: yield Item B\n\n</code></pre> <p>The asyncio event loop handles I/O operations while each pipeline stage runs in its own thread for true CPU parallelism. This design enables:</p> <ul> <li>Pipeline Parallelism: Multiple items can be processed simultaneously at different stages</li> <li>Order Preservation: Output maintains the same order as input despite parallel processing</li> <li>Async I/O: Non-blocking input and output operations</li> <li>CPU Efficiency: True parallelism for CPU-bound tasks through threading</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Data Processing: Transform streaming data with CPU-intensive operations</li> <li>ETL Pipelines: Extract, transform, and load data with performance monitoring</li> <li>Machine Learning: Process training data or inference requests</li> <li>Image/Video Processing: Handle media files with CPU-bound transformations</li> <li>Log Analysis: Process log streams in real-time</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install async-task-pipeline\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Check out the API Reference for detailed documentation</li> <li>Explore the pipeline components: Pipeline, Stage, Item</li> <li>Learn about utilities for performance analysis and logging</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This page provides comprehensive examples of using the Async Task Pipeline framework for various use cases.</p>"},{"location":"examples/#pipeline-architecture-overview","title":"Pipeline Architecture Overview","text":"<p>The following diagram shows the basic structure of a pipeline with three processing stages:</p> <pre><code>graph LR\n    A[Async Input Stream] --&gt; B[Input Queue]\n    B --&gt; C[Stage 1: Process]\n    C --&gt; D[Queue 1]\n    D --&gt; E[Stage 2: Filter]\n    E --&gt; F[Queue 2]\n    F --&gt; G[Stage 3: Transform]\n    G --&gt; H[Output Queue]\n    H --&gt; I[Async Output Stream]\n\n    subgraph \"Thread Pool\"\n        C\n        E\n        G\n    end\n\n    subgraph \"Main Thread (Asyncio)\"\n        A\n        I\n    end\n</code></pre>"},{"location":"examples/#basic-pipeline","title":"Basic Pipeline","text":"<p>Here's a simple example that demonstrates the core functionality:</p> <pre><code>import asyncio\nfrom async_task_pipeline import AsyncTaskPipeline\n\nasync def basic_example():\n    # Create pipeline with timing enabled\n    pipeline = AsyncTaskPipeline(max_queue_size=50, enable_timing=True)\n\n    # Add processing stages\n    pipeline.add_stage(\"multiply\", lambda x: x * 2)\n    pipeline.add_stage(\"filter_even\", lambda x: x if x % 4 == 0 else None)\n    pipeline.add_stage(\"stringify\", lambda x: f\"Result: {x}\")\n\n    # Start the pipeline\n    await pipeline.start()\n\n    # Create input data\n    async def data_generator():\n        for i in range(1, 21):\n            yield i\n            await asyncio.sleep(0.01)  # Simulate streaming data\n\n    # Process the input stream\n    await pipeline.process_input_stream(data_generator())\n\n    # Collect results\n    results = []\n    async for result in pipeline.generate_output_stream():\n        results.append(result)\n\n    # Get performance statistics\n    stats = pipeline.get_latency_summary()\n    print(f\"Processed {stats['total_items']} items\")\n    print(f\"Average latency: {stats['avg_total_latency']:.3f}s\")\n\n    # Stop the pipeline\n    await pipeline.stop()\n\n    return results\n\n# Run the example\n# results = asyncio.run(basic_example())\n# print(results)  # ['Result: 4', 'Result: 8', 'Result: 12', ...]\n</code></pre>"},{"location":"examples/#cpu-intensive-processing","title":"CPU-Intensive Processing","text":"<p>Example with CPU-bound tasks like image processing or data transformation:</p> <pre><code>import asyncio\nimport time\nfrom async_task_pipeline import AsyncTaskPipeline\n\ndef cpu_intensive_task(data):\n    \"\"\"Simulate CPU-intensive processing\"\"\"\n    # Simulate heavy computation\n    result = 0\n    for i in range(data * 1000):\n        result += i ** 0.5\n    return result\n\ndef validate_result(data):\n    \"\"\"Validation stage\"\"\"\n    return data if data &gt; 1000 else None\n\nasync def cpu_intensive_example():\n    pipeline = AsyncTaskPipeline(max_queue_size=10, enable_timing=True)\n\n    # Add CPU-intensive stages\n    pipeline.add_stage(\"compute\", cpu_intensive_task)\n    pipeline.add_stage(\"validate\", validate_result)\n    pipeline.add_stage(\"round\", lambda x: round(x, 2))\n\n    await pipeline.start()\n\n    # Generate input data\n    async def workload_generator():\n        for i in range(1, 11):\n            yield i\n            await asyncio.sleep(0.1)\n\n    # Process workload\n    await pipeline.process_input_stream(workload_generator())\n\n    results = []\n    async for result in pipeline.generate_output_stream():\n        results.append(result)\n\n    # Analyze performance\n    stats = pipeline.get_latency_summary()\n    print(f\"Total items: {stats['total_items']}\")\n    print(f\"Average latency: {stats['avg_total_latency']:.3f}s\")\n\n    for stage_name, stage_stats in stats['stage_statistics'].items():\n        timing = stage_stats['timing_breakdown']\n        print(f\"{stage_name}:\")\n        print(f\"  Computation ratio: {timing['computation_ratio']:.1%}\")\n        print(f\"  Avg processing time: {timing['avg_computation_time']*1000:.1f}ms\")\n\n    await pipeline.stop()\n    return results\n</code></pre>"},{"location":"examples/#data-transformation-pipeline","title":"Data Transformation Pipeline","text":"<p>Example for ETL-style data processing:</p> <pre><code>import asyncio\nimport json\nfrom typing import Dict, Any\nfrom async_task_pipeline import AsyncTaskPipeline\n\ndef parse_json(raw_data: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse JSON data\"\"\"\n    try:\n        return json.loads(raw_data)\n    except json.JSONDecodeError:\n        return None\n\ndef validate_data(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Validate required fields\"\"\"\n    required_fields = ['id', 'name', 'value']\n    if all(field in data for field in required_fields):\n        return data\n    return None\n\ndef transform_data(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Transform and enrich data\"\"\"\n    return {\n        'id': data['id'],\n        'name': data['name'].upper(),\n        'value': data['value'] * 1.1,  # Apply 10% markup\n        'processed_at': time.time()\n    }\n\nasync def etl_example():\n    pipeline = AsyncTaskPipeline(max_queue_size=100, enable_timing=True)\n\n    # Build ETL pipeline\n    pipeline.add_stage(\"parse\", parse_json)\n    pipeline.add_stage(\"validate\", validate_data)\n    pipeline.add_stage(\"transform\", transform_data)\n\n    await pipeline.start()\n\n    # Simulate streaming JSON data\n    sample_data = [\n        '{\"id\": 1, \"name\": \"item1\", \"value\": 100}',\n        '{\"id\": 2, \"name\": \"item2\", \"value\": 200}',\n        '{\"invalid\": \"json\"}',  # This will be filtered out\n        '{\"id\": 3, \"name\": \"item3\", \"value\": 300}',\n    ]\n\n    async def json_stream():\n        for json_str in sample_data:\n            yield json_str\n            await asyncio.sleep(0.05)\n\n    await pipeline.process_input_stream(json_stream())\n\n    results = []\n    async for result in pipeline.generate_output_stream():\n        results.append(result)\n\n    await pipeline.stop()\n    return results\n</code></pre>"},{"location":"examples/#generator-based-processing","title":"Generator-Based Processing","text":"<p>Example where stages can produce multiple outputs:</p> <pre><code>import asyncio\nfrom async_task_pipeline import AsyncTaskPipeline\n\ndef split_text(text: str):\n    \"\"\"Split text into words (generator)\"\"\"\n    for word in text.split():\n        yield word.strip('.,!?')\n\ndef filter_long_words(word: str):\n    \"\"\"Filter words longer than 3 characters\"\"\"\n    return word if len(word) &gt; 3 else None\n\ndef uppercase_word(word: str):\n    \"\"\"Convert to uppercase\"\"\"\n    return word.upper()\n\nasync def text_processing_example():\n    pipeline = AsyncTaskPipeline(max_queue_size=50, enable_timing=True)\n\n    # Text processing pipeline\n    pipeline.add_stage(\"split\", split_text)\n    pipeline.add_stage(\"filter\", filter_long_words)\n    pipeline.add_stage(\"uppercase\", uppercase_word)\n\n    await pipeline.start()\n\n    # Input sentences\n    sentences = [\n        \"The quick brown fox jumps over the lazy dog\",\n        \"Python is a great programming language\",\n        \"Async processing makes things fast\"\n    ]\n\n    async def sentence_stream():\n        for sentence in sentences:\n            yield sentence\n            await asyncio.sleep(0.1)\n\n    await pipeline.process_input_stream(sentence_stream())\n\n    words = []\n    async for word in pipeline.generate_output_stream():\n        words.append(word)\n\n    await pipeline.stop()\n    return words\n</code></pre>"},{"location":"examples/#performance-analysis","title":"Performance Analysis","text":"<p>Example focusing on performance monitoring and analysis:</p> <pre><code>import asyncio\nfrom async_task_pipeline import AsyncTaskPipeline, log_pipeline_performance_analysis\n\ndef slow_stage(data):\n    \"\"\"Intentionally slow stage for demonstration\"\"\"\n    time.sleep(0.1)  # Simulate slow processing\n    return data * 2\n\ndef fast_stage(data):\n    \"\"\"Fast processing stage\"\"\"\n    return data + 1\n\nasync def performance_analysis_example():\n    # Enable detailed timing\n    pipeline = AsyncTaskPipeline(max_queue_size=20, enable_timing=True)\n\n    pipeline.add_stage(\"slow_process\", slow_stage)\n    pipeline.add_stage(\"fast_process\", fast_stage)\n    pipeline.add_stage(\"final_transform\", lambda x: f\"Final: {x}\")\n\n    await pipeline.start()\n\n    # Process data\n    async def data_stream():\n        for i in range(10):\n            yield i\n            await asyncio.sleep(0.02)\n\n    await pipeline.process_input_stream(data_stream())\n\n    results = []\n    async for result in pipeline.generate_output_stream():\n        results.append(result)\n\n    # Detailed performance analysis\n    log_pipeline_performance_analysis(pipeline)\n\n    # Manual analysis\n    stats = pipeline.get_latency_summary()\n    print(f\"\\nManual Analysis:\")\n    print(f\"Overall computation efficiency: {stats['overall_efficiency']['computation_efficiency']:.1%}\")\n\n    for stage_name, stage_stats in stats['stage_statistics'].items():\n        timing = stage_stats['timing_breakdown']\n        print(f\"{stage_name}: {timing['computation_ratio']:.1%} computation\")\n\n    await pipeline.stop()\n    return results\n\n# Run performance analysis\n# asyncio.run(performance_analysis_example())\n</code></pre>"},{"location":"examples/#error-handling","title":"Error Handling","text":"<p>Example with robust error handling:</p> <pre><code>import asyncio\nimport logging\nfrom async_task_pipeline import AsyncTaskPipeline, logger\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\n\ndef risky_operation(data):\n    \"\"\"Operation that might fail\"\"\"\n    if data % 3 == 0:\n        raise ValueError(f\"Cannot process {data}\")\n    return data * 2\n\ndef safe_wrapper(func):\n    \"\"\"Wrapper to handle errors gracefully\"\"\"\n    def wrapper(data):\n        try:\n            return func(data)\n        except Exception as e:\n            logger.error(f\"Error processing {data}: {e}\")\n            return None  # Filter out failed items\n    return wrapper\n\nasync def error_handling_example():\n    pipeline = AsyncTaskPipeline(max_queue_size=30, enable_timing=True)\n\n    # Wrap risky operations\n    pipeline.add_stage(\"risky\", safe_wrapper(risky_operation))\n    pipeline.add_stage(\"safe\", lambda x: x + 10)\n\n    await pipeline.start()\n\n    # Input data that will cause some failures\n    async def data_with_errors():\n        for i in range(1, 11):\n            yield i\n            await asyncio.sleep(0.05)\n\n    await pipeline.process_input_stream(data_with_errors())\n\n    results = []\n    async for result in pipeline.generate_output_stream():\n        results.append(result)\n\n    print(f\"Successfully processed {len(results)} items\")\n    print(f\"Results: {results}\")\n\n    await pipeline.stop()\n    return results\n</code></pre>"},{"location":"examples/#best-practices","title":"Best Practices","text":""},{"location":"examples/#1-resource-management","title":"1. Resource Management","text":"<p>Always use proper resource management:</p> <pre><code>async def resource_managed_pipeline():\n    pipeline = AsyncTaskPipeline(enable_timing=True)\n\n    try:\n        # Setup pipeline\n        pipeline.add_stage(\"process\", lambda x: x * 2)\n        await pipeline.start()\n\n        # Process data\n        # ... your processing logic ...\n\n    finally:\n        # Always clean up\n        await pipeline.stop()\n</code></pre>"},{"location":"examples/#2-monitoring-and-logging","title":"2. Monitoring and Logging","text":"<p>Enable comprehensive monitoring:</p> <pre><code>import logging\nfrom async_task_pipeline import logger, log_pipeline_performance_analysis\n\n# Configure detailed logging\nlogger.setLevel(logging.DEBUG)\nhandler = logging.StreamHandler()\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n# Use performance analysis\nasync def monitored_pipeline():\n    pipeline = AsyncTaskPipeline(enable_timing=True)\n    # ... setup and processing ...\n\n    # Analyze performance\n    log_pipeline_performance_analysis(pipeline)\n</code></pre>"},{"location":"examples/#3-queue-sizing","title":"3. Queue Sizing","text":"<p>Choose appropriate queue sizes based on your use case:</p> <pre><code># For memory-constrained environments\npipeline = AsyncTaskPipeline(max_queue_size=10)\n\n# For high-throughput scenarios\npipeline = AsyncTaskPipeline(max_queue_size=1000)\n\n# For balanced performance\npipeline = AsyncTaskPipeline(max_queue_size=100)  # Default\n</code></pre> <p>These examples demonstrate the flexibility and power of the Async Task Pipeline framework for various data processing scenarios.</p>"},{"location":"api/item/","title":"Item","text":"<p>Data containers that flow through the pipeline with timing and sequence tracking.</p>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem","title":"<code>async_task_pipeline.base.item.PipelineItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for data flowing through pipeline with sequence and timing tracking.</p> <p>A wrapper class that carries data through the pipeline along with metadata for tracking sequence, timing, and performance analysis. Each item maintains detailed timing information as it flows through different pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>seq_num</code> <code>int</code> <p>Unique sequence number identifying this item's position in the input stream.</p> required <code>data</code> <code>DataT</code> <p>The actual data payload being processed through the pipeline.</p> required <code>enable_timing</code> <code>bool</code> <p>Whether to collect detailed timing information for performance analysis.</p> <code>True</code> <code>start_timestamp</code> <code>float</code> <p>Timestamp when the item entered the pipeline. Auto-generated if not provided.</p> required <p>Attributes:</p> Name Type Description <code>_stage_timestamps</code> <code>dict[str, float]</code> <p>Timestamps when each stage completed processing this item.</p> <code>_detailed_timings</code> <code>dict[str, DetailedTiming]</code> <p>Detailed timing breakdowns for each stage.</p> <code>_queue_enter_times</code> <code>dict[str, float]</code> <p>Timestamps when the item entered each stage's input queue.</p>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.model_post_init","title":"<code>model_post_init(context)</code>","text":"<p>Initialize the item</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>def model_post_init(self, context: Any) -&gt; None:\n    \"\"\"Initialize the item\"\"\"\n    self._stage_timestamps = {}\n    self._detailed_timings = {}\n    self._queue_enter_times = {}\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.record_queue_entry","title":"<code>record_queue_entry(stage_name)</code>","text":"<p>Record when item enters a stage's input queue.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Name of the stage whose queue the item is entering.</p> required Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef record_queue_entry(self, stage_name: str) -&gt; None:\n    \"\"\"Record when item enters a stage's input queue.\n\n    Parameters\n    ----------\n    stage_name : str\n        Name of the stage whose queue the item is entering.\n    \"\"\"\n    self._queue_enter_times[stage_name] = time.perf_counter()\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.record_stage_completion","title":"<code>record_stage_completion(stage_name)</code>","text":"<p>Record when a stage completes processing this item.</p> <p>Parameters:</p> Name Type Description Default <code>stage_name</code> <code>str</code> <p>Name of the stage that completed processing.</p> required Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef record_stage_completion(self, stage_name: str) -&gt; None:\n    \"\"\"Record when a stage completes processing this item.\n\n    Parameters\n    ----------\n    stage_name : str\n        Name of the stage that completed processing.\n    \"\"\"\n    self._stage_timestamps[stage_name] = time.perf_counter()\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.record_detailed_timing","title":"<code>record_detailed_timing(stage_name, detailed_timing)</code>","text":"<p>Record detailed timing for a stage</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef record_detailed_timing(self, stage_name: str, detailed_timing: DetailedTiming) -&gt; None:\n    \"\"\"Record detailed timing for a stage\"\"\"\n    self._detailed_timings[stage_name] = detailed_timing\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_queue_enter_time","title":"<code>get_queue_enter_time(stage_name)</code>","text":"<p>Get the time the item entered the queue for a stage</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_queue_enter_time(self, stage_name: str) -&gt; float | None:\n    \"\"\"Get the time the item entered the queue for a stage\"\"\"\n    if stage_name not in self._queue_enter_times:\n        return None\n    return self._queue_enter_times[stage_name]\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_stage_completion_time","title":"<code>get_stage_completion_time(stage_name)</code>","text":"<p>Get the time the item completed processing for a stage</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_stage_completion_time(self, stage_name: str) -&gt; float | None:\n    \"\"\"Get the time the item completed processing for a stage\"\"\"\n    if stage_name not in self._stage_timestamps:\n        return None\n    return self._stage_timestamps[stage_name]\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_detailed_timing","title":"<code>get_detailed_timing(stage_name)</code>","text":"<p>Get the detailed timing for a stage</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_detailed_timing(self, stage_name: str) -&gt; DetailedTiming | None:\n    \"\"\"Get the detailed timing for a stage\"\"\"\n    if stage_name not in self._detailed_timings:\n        return None\n    return self._detailed_timings[stage_name]\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_total_latency","title":"<code>get_total_latency()</code>","text":"<p>Calculate total end-to-end latency.</p> <p>Computes the time from when the item entered the pipeline until the last stage completed processing it.</p> <p>Returns:</p> Type Description <code>float | None</code> <p>Total latency in seconds, or None if timing is disabled or no stages have completed processing.</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_total_latency(self) -&gt; float | None:\n    \"\"\"Calculate total end-to-end latency.\n\n    Computes the time from when the item entered the pipeline until\n    the last stage completed processing it.\n\n    Returns\n    -------\n    float | None\n        Total latency in seconds, or None if timing is disabled or\n        no stages have completed processing.\n    \"\"\"\n    if not self._stage_timestamps or self.start_timestamp is None:\n        return None\n\n    last_timestamp = max(self._stage_timestamps.values())\n    return last_timestamp - self.start_timestamp\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_stage_latencies","title":"<code>get_stage_latencies()</code>","text":"<p>Calculate latency for each stage</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_stage_latencies(self) -&gt; dict[str, float] | None:\n    \"\"\"Calculate latency for each stage\"\"\"\n    if not self._stage_timestamps or self.start_timestamp is None:\n        return None\n    latencies: dict[str, float] = {}\n    sorted_stages = sorted(self._stage_timestamps.items(), key=lambda x: x[1])\n\n    prev_time = self.start_timestamp\n    for stage_name, timestamp in sorted_stages:\n        latencies[stage_name] = timestamp - prev_time\n        prev_time = timestamp\n\n    return latencies\n</code></pre>"},{"location":"api/item/#async_task_pipeline.base.item.PipelineItem.get_timing_breakdown","title":"<code>get_timing_breakdown()</code>","text":"<p>Get detailed timing breakdown for each stage.</p> <p>Provides comprehensive timing analysis including queue wait times, computation times, transmission times, and overall efficiency metrics.</p> <p>Returns:</p> Type Description <code>dict[str, dict[str, float]] | None</code> <p>Dictionary with per-stage timing breakdowns and totals, including: - Per-stage: queue_wait_time, computation_time, transmission_time - Totals: total_computation_time, total_overhead_time, computation_ratio Returns None if timing is disabled or no detailed timings available.</p> Source code in <code>src/async_task_pipeline/base/item.py</code> <pre><code>@_if_timing_enabled\ndef get_timing_breakdown(self) -&gt; dict[str, dict[str, float]] | None:\n    \"\"\"Get detailed timing breakdown for each stage.\n\n    Provides comprehensive timing analysis including queue wait times,\n    computation times, transmission times, and overall efficiency metrics.\n\n    Returns\n    -------\n    dict[str, dict[str, float]] | None\n        Dictionary with per-stage timing breakdowns and totals, including:\n        - Per-stage: queue_wait_time, computation_time, transmission_time\n        - Totals: total_computation_time, total_overhead_time, computation_ratio\n        Returns None if timing is disabled or no detailed timings available.\n    \"\"\"\n    if not self._detailed_timings or self.start_timestamp is None:\n        return None\n\n    breakdown: dict[str, dict[str, float]] = {\n        stage_name: {\n            \"queue_wait_time\": timing.queue_wait_time,\n            \"computation_time\": timing.computation_time,\n            \"transmission_time\": timing.transmission_time,\n            \"total_stage_time\": timing.queue_wait_time + timing.computation_time + timing.transmission_time,\n        }\n        for stage_name, timing in self._detailed_timings.items()\n    }\n    total_latency = self.get_total_latency()\n    events: list[tuple[float, str, str | None]] = [(self.start_timestamp, \"start\", None)]\n\n    for stage_name, timing in self._detailed_timings.items():\n        events.extend(\n            (\n                (timing.processing_start_time, \"compute_start\", stage_name),\n                (timing.processing_end_time, \"compute_end\", stage_name),\n            )\n        )\n    events.sort(key=lambda x: x[0])\n\n    total_computation_time = 0.0\n    last_time = self.start_timestamp\n    computing_stages: set[str | None] = set()\n\n    for event_time, event_type, _stage_name in events:\n        if computing_stages:\n            total_computation_time += event_time - last_time\n\n        if event_type == \"compute_start\":\n            computing_stages.add(_stage_name)\n        elif event_type == \"compute_end\":\n            computing_stages.discard(_stage_name)\n\n        last_time = event_time\n\n    end_time = self.start_timestamp + total_latency if total_latency else 0.0\n    if computing_stages and last_time &lt; end_time:\n        total_computation_time += end_time - last_time\n\n    total_overhead_time = total_latency - total_computation_time if total_latency else 0.0\n\n    if breakdown:\n        breakdown[\"totals\"] = {\n            \"total_computation_time\": total_computation_time,\n            \"total_overhead_time\": total_overhead_time,\n            \"total_latency\": total_latency if total_latency is not None else 0.0,\n            \"computation_ratio\": (total_computation_time / total_latency) if total_latency else 0.0,\n        }\n\n    return breakdown\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline","text":"<p>The main pipeline orchestrator that manages data flow through multiple processing stages.</p>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline","title":"<code>async_task_pipeline.base.pipeline.AsyncTaskPipeline(max_queue_size=100, enable_timing=False)</code>","text":"<p>Main pipeline orchestrator with async I/O and thread-based processing.</p> <p>This class manages a multi-stage data processing pipeline that combines async I/O for input/output operations with thread-based processing for CPU-intensive tasks. It provides comprehensive timing analysis and performance monitoring capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>max_queue_size</code> <code>int</code> <p>Maximum size for inter-stage queues. Controls memory usage and backpressure.</p> <code>100</code> <code>enable_timing</code> <code>bool</code> <p>Whether to enable detailed timing analysis for performance monitoring.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pipeline = AsyncTaskPipeline(max_queue_size=50, enable_timing=True)\n&gt;&gt;&gt; pipeline.add_stage(\"process\", lambda x: x * 2)\n&gt;&gt;&gt; await pipeline.start()\n</code></pre> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def __init__(self, max_queue_size: int = 100, enable_timing: bool = False):\n    self.max_queue_size = max_queue_size\n    self.stages: list[PipelineStage] = []\n    self.queues: list[queue.Queue[PipelineItem[T] | U]] = []\n    self.input_queue: queue.Queue[PipelineItem[T] | U] | None = None\n    self.output_queue: queue.Queue[PipelineItem[T] | U] | None = None\n    self.running = False\n    self.sequence_counter = 0\n    self.completed_items: list[PipelineItem[T]] = []\n    self.enable_timing = enable_timing\n    self._sleep_time = 0.001\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.add_stage","title":"<code>add_stage(name, process_fn)</code>","text":"<p>Add a processing stage to the pipeline.</p> <p>Creates a new stage with the specified processing function and connects it to the pipeline's queue system. Stages are executed in the order they are added.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for this stage, used in timing analysis and logging.</p> required <code>process_fn</code> <code>Callable</code> <p>Function to process data items. Should accept a single argument and return processed data, None (to filter), or a generator (to produce multiple outputs).</p> required Notes <p>The process_fn should be thread-safe as it will be executed in a separate thread. If the function returns None, the item is filtered out. If it returns a generator, each yielded value becomes a separate pipeline item.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def add_stage(self, name: str, process_fn: Callable) -&gt; None:\n    \"\"\"Add a processing stage to the pipeline.\n\n    Creates a new stage with the specified processing function and connects\n    it to the pipeline's queue system. Stages are executed in the order\n    they are added.\n\n    Parameters\n    ----------\n    name : str\n        Unique identifier for this stage, used in timing analysis and logging.\n    process_fn : Callable\n        Function to process data items. Should accept a single argument and\n        return processed data, None (to filter), or a generator (to produce\n        multiple outputs).\n\n    Notes\n    -----\n    The process_fn should be thread-safe as it will be executed in a\n    separate thread. If the function returns None, the item is filtered\n    out. If it returns a generator, each yielded value becomes a separate\n    pipeline item.\n    \"\"\"\n    if not self.queues:\n        self.input_queue = queue.Queue(maxsize=self.max_queue_size)\n        input_q = self.input_queue\n    else:\n        input_q = self.queues[-1]\n\n    output_q: queue.Queue[PipelineItem[T] | U] = queue.Queue(maxsize=self.max_queue_size)\n    self.queues.append(output_q)\n    self.output_queue = output_q\n    stage = PipelineStage(name, process_fn, input_q, output_q, enable_timing=self.enable_timing)\n    self.stages.append(stage)\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start all pipeline stages.</p> <p>Initializes and starts worker threads for all registered stages. The pipeline must be started before processing any data.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the pipeline is already running or if no stages have been added.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>async def start(self) -&gt; None:\n    \"\"\"Start all pipeline stages.\n\n    Initializes and starts worker threads for all registered stages.\n    The pipeline must be started before processing any data.\n\n    Raises\n    ------\n    RuntimeError\n        If the pipeline is already running or if no stages have been added.\n    \"\"\"\n    self.running = True\n    for stage in self.stages:\n        stage.start()\n    logger.info(\"Pipeline started\")\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop all pipeline stages.</p> <p>Gracefully shuts down all worker threads and clears pipeline state. This method should be called when pipeline processing is complete.</p> Notes <p>Stages are stopped in reverse order to ensure proper cleanup. Any remaining items in queues will be lost.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Stop all pipeline stages.\n\n    Gracefully shuts down all worker threads and clears pipeline state.\n    This method should be called when pipeline processing is complete.\n\n    Notes\n    -----\n    Stages are stopped in reverse order to ensure proper cleanup.\n    Any remaining items in queues will be lost.\n    \"\"\"\n    self.running = False\n    for stage in reversed(self.stages):\n        stage.stop()\n    logger.info(\"Pipeline stopped\")\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.process_input_stream","title":"<code>process_input_stream(input_stream)</code>  <code>async</code>","text":"<p>Consume async input stream and feed to pipeline.</p> <p>Processes an async iterator/generator and feeds each item into the pipeline for processing. This method handles the async-to-sync bridge for pipeline input.</p> <p>Parameters:</p> Name Type Description Default <code>input_stream</code> <code>AsyncIterator[Any]</code> <p>Async iterator that yields data items to be processed.</p> required Notes <p>This method will consume the entire input stream. For continuous processing, use individual <code>process_input_data</code> calls.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>async def process_input_stream(self, input_stream: AsyncIterator[Any]) -&gt; None:\n    \"\"\"Consume async input stream and feed to pipeline.\n\n    Processes an async iterator/generator and feeds each item into the\n    pipeline for processing. This method handles the async-to-sync\n    bridge for pipeline input.\n\n    Parameters\n    ----------\n    input_stream : AsyncIterator[Any]\n        Async iterator that yields data items to be processed.\n\n    Notes\n    -----\n    This method will consume the entire input stream. For continuous\n    processing, use individual `process_input_data` calls.\n    \"\"\"\n    try:\n        async for data in input_stream:\n            await self.process_input_data(data, time.perf_counter())\n    except Exception as e:\n        logger.error(f\"Error processing input stream: {e}\")\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.interrupt","title":"<code>interrupt()</code>  <code>async</code>","text":"<p>Interrupt the pipeline</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>async def interrupt(self) -&gt; None:\n    \"\"\"Interrupt the pipeline\"\"\"\n    if not self.running:\n        return\n    self.clear()\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.generate_output_stream","title":"<code>generate_output_stream()</code>  <code>async</code>","text":"<p>Generate async output stream from pipeline, maintaining order.</p> <p>Creates an async iterator that yields processed items as they become available from the pipeline. Items are yielded in the order they were processed (which may differ from input order due to parallel processing).</p> <p>Yields:</p> Type Description <code>T | U</code> <p>Processed data items or sentinel values from the pipeline.</p> Notes <p>This method will continue yielding items until the pipeline is stopped and all queues are empty. It's typically used in an async for loop.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>async def generate_output_stream(self) -&gt; AsyncIterator[T | U]:\n    \"\"\"Generate async output stream from pipeline, maintaining order.\n\n    Creates an async iterator that yields processed items as they become\n    available from the pipeline. Items are yielded in the order they\n    were processed (which may differ from input order due to parallel\n    processing).\n\n    Yields\n    ------\n    T | U\n        Processed data items or sentinel values from the pipeline.\n\n    Notes\n    -----\n    This method will continue yielding items until the pipeline is stopped\n    and all queues are empty. It's typically used in an async for loop.\n    \"\"\"\n    while self.running or (self.output_queue and not self.output_queue.empty()):\n        try:\n            item = await asyncio.get_event_loop().run_in_executor(None, self._get_output_nowait)\n            if item is None:\n                await asyncio.sleep(self._sleep_time)\n                continue\n\n            if isinstance(item, PipelineItem):\n                yield item.data\n                self.completed_items.append(item)\n            else:\n                yield item\n\n        except Exception as e:\n            logger.error(f\"Error generating output: {e}\")\n            await asyncio.sleep(self._sleep_time)\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.get_latency_summary","title":"<code>get_latency_summary()</code>","text":"<p>Get summary statistics for pipeline latency.</p> <p>Computes comprehensive performance statistics including end-to-end latency, per-stage timing breakdowns, and efficiency metrics.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing: - total_items: Number of processed items - avg_total_latency: Average end-to-end latency in seconds - min_total_latency: Minimum latency observed - max_total_latency: Maximum latency observed - stage_statistics: Per-stage performance metrics - overall_efficiency: Computation vs overhead ratios</p> Notes <p>Only available when timing is enabled. Returns empty dict if no items have been processed or timing is disabled.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def get_latency_summary(self) -&gt; dict[str, Any]:\n    \"\"\"Get summary statistics for pipeline latency.\n\n    Computes comprehensive performance statistics including end-to-end\n    latency, per-stage timing breakdowns, and efficiency metrics.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary containing:\n        - total_items: Number of processed items\n        - avg_total_latency: Average end-to-end latency in seconds\n        - min_total_latency: Minimum latency observed\n        - max_total_latency: Maximum latency observed\n        - stage_statistics: Per-stage performance metrics\n        - overall_efficiency: Computation vs overhead ratios\n\n    Notes\n    -----\n    Only available when timing is enabled. Returns empty dict if no\n    items have been processed or timing is disabled.\n    \"\"\"\n    if not self.completed_items:\n        return {}\n\n    total_latencies = [\n        latency for item in self.completed_items if (latency := item.get_total_latency()) is not None\n    ]\n    avg_latency = sum(total_latencies) / len(total_latencies) if total_latencies else 0.0\n    min_latency = min(total_latencies, default=0.0)\n    max_latency = max(total_latencies, default=0.0)\n\n    stage_stats: dict[str, Any] = {}\n\n    total_computation_ratios = []\n\n    for stage in self.stages:\n        stage_latencies: list[float] = []\n        stage_computation_times: list[float] = []\n        stage_queue_wait_times: list[float] = []\n        stage_transmission_times: list[float] = []\n\n        for item in self.completed_items:\n            stage_latencies_dict = item.get_stage_latencies()\n            if stage_latencies_dict is not None and stage.name in stage_latencies_dict:\n                stage_latencies.append(stage_latencies_dict[stage.name])\n\n            if (timing := item.get_detailed_timing(stage.name)) is not None:\n                stage_computation_times.append(max(timing.computation_time, 0.0))\n                stage_queue_wait_times.append(max(timing.queue_wait_time, 0.0))\n                stage_transmission_times.append(max(timing.transmission_time, 0.0))\n\n        if stage_latencies:\n            avg_computation = (\n                sum(stage_computation_times) / len(stage_computation_times) if stage_computation_times else 0.0\n            )\n            avg_queue_wait = (\n                sum(stage_queue_wait_times) / len(stage_queue_wait_times) if stage_queue_wait_times else 0.0\n            )\n            avg_transmission = (\n                sum(stage_transmission_times) / len(stage_transmission_times) if stage_transmission_times else 0.0\n            )\n\n            stage_stats[stage.name] = {\n                \"avg_latency\": sum(stage_latencies) / len(stage_latencies),\n                \"min_latency\": min(stage_latencies),\n                \"max_latency\": max(stage_latencies),\n                \"processed_count\": stage.processed_count,\n                \"avg_processing_time\": stage.get_average_processing_time(),\n                \"timing_breakdown\": {\n                    \"avg_computation_time\": avg_computation,\n                    \"avg_queue_wait_time\": avg_queue_wait,\n                    \"avg_transmission_time\": avg_transmission,\n                    \"computation_ratio\": avg_computation / (avg_computation + avg_queue_wait + avg_transmission)\n                    if (avg_computation + avg_queue_wait + avg_transmission) &gt; 0\n                    else 0.0,\n                },\n            }\n\n    for item in self.completed_items:\n        if (breakdown := item.get_timing_breakdown()) is not None and \"totals\" in breakdown:\n            total_computation_ratios.append(breakdown[\"totals\"][\"computation_ratio\"])\n\n    avg_computation_ratio = (\n        sum(total_computation_ratios) / len(total_computation_ratios) if total_computation_ratios else 0.0\n    )\n\n    return {\n        \"total_items\": len(self.completed_items),\n        \"avg_total_latency\": avg_latency,\n        \"min_total_latency\": min_latency,\n        \"max_total_latency\": max_latency,\n        \"stage_statistics\": stage_stats,\n        \"overall_efficiency\": {\n            \"computation_efficiency\": avg_computation_ratio,\n            \"overhead_ratio\": 1.0 - avg_computation_ratio,\n        },\n    }\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.clear","title":"<code>clear()</code>","text":"<p>Clear the pipeline state and queues.</p> <p>Removes all items from input/output queues and stage queues, resets completed items list, and resets the sequence counter. This method is useful for resetting the pipeline between runs.</p> Notes <p>This method should only be called when the pipeline is stopped. Any items currently being processed may be lost.</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the pipeline state and queues.\n\n    Removes all items from input/output queues and stage queues,\n    resets completed items list, and resets the sequence counter.\n    This method is useful for resetting the pipeline between runs.\n\n    Notes\n    -----\n    This method should only be called when the pipeline is stopped.\n    Any items currently being processed may be lost.\n    \"\"\"\n    self.clear_input_queue()\n    self.clear_output_queue()\n\n    for stage in self.stages:\n        stage.clear_input_queue()\n\n    self.completed_items = []\n    self.sequence_counter = 0\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.clear_input_queue","title":"<code>clear_input_queue()</code>","text":"<p>Clear the input queue</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def clear_input_queue(self) -&gt; None:\n    \"\"\"Clear the input queue\"\"\"\n    if self.input_queue is not None:\n        while not self.input_queue.empty():\n            self.input_queue.get()\n</code></pre>"},{"location":"api/pipeline/#async_task_pipeline.base.pipeline.AsyncTaskPipeline.clear_output_queue","title":"<code>clear_output_queue()</code>","text":"<p>Clear the output queue</p> Source code in <code>src/async_task_pipeline/base/pipeline.py</code> <pre><code>def clear_output_queue(self) -&gt; None:\n    \"\"\"Clear the output queue\"\"\"\n    if self.output_queue is not None:\n        while not self.output_queue.empty():\n            self.output_queue.get()\n</code></pre>"},{"location":"api/stage/","title":"Stage","text":"<p>Individual processing stages that execute CPU-bound tasks in separate threads.</p>"},{"location":"api/stage/#async_task_pipeline.base.stage.PipelineStage","title":"<code>async_task_pipeline.base.stage.PipelineStage(name, process_fn, input_queue, output_queue, enable_timing=True)</code>","text":"<p>Single stage in the CPU-intensive task pipeline.</p> <p>Represents an individual processing stage that runs in its own thread and processes items from an input queue, applying a transformation function, and placing results in an output queue.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for this stage, used in logging and timing analysis.</p> required <code>process_fn</code> <code>Callable</code> <p>Function to process data items. Should be thread-safe and accept a single argument, returning processed data, None (to filter), or a generator (for multiple outputs).</p> required <code>input_queue</code> <code>Queue</code> <p>Queue from which to read input items for processing.</p> required <code>output_queue</code> <code>Queue</code> <p>Queue to which processed items are written.</p> required <code>enable_timing</code> <code>bool</code> <p>Whether to collect detailed timing information for this stage.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>processed_count</code> <code>int</code> <p>Number of items successfully processed by this stage.</p> <code>total_processing_time</code> <code>float</code> <p>Total time spent in processing function (when timing enabled).</p> Source code in <code>src/async_task_pipeline/base/stage.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    process_fn: Callable,\n    input_queue: queue.Queue,\n    output_queue: queue.Queue,\n    enable_timing: bool = True,\n):\n    self.name = name\n    self.process_fn = process_fn\n    self.input_queue = input_queue\n    self.output_queue = output_queue\n    self.thread: threading.Thread | None = None\n    self.running = False\n    self.processed_count = 0\n    self.total_processing_time = 0.0\n    self.enable_timing = enable_timing\n</code></pre>"},{"location":"api/stage/#async_task_pipeline.base.stage.PipelineStage.start","title":"<code>start()</code>","text":"<p>Start the worker thread for this stage.</p> <p>Creates and starts a daemon thread that will continuously process items from the input queue until stopped.</p> Notes <p>The worker thread is marked as daemon so it won't prevent the program from exiting.</p> Source code in <code>src/async_task_pipeline/base/stage.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the worker thread for this stage.\n\n    Creates and starts a daemon thread that will continuously process\n    items from the input queue until stopped.\n\n    Notes\n    -----\n    The worker thread is marked as daemon so it won't prevent the\n    program from exiting.\n    \"\"\"\n    self.running = True\n    self.thread = threading.Thread(target=self._worker, name=f\"Stage-{self.name}\")\n    self.thread.daemon = True\n    self.thread.start()\n    logger.info(f\"Started pipeline stage: {self.name}\")\n</code></pre>"},{"location":"api/stage/#async_task_pipeline.base.stage.PipelineStage.stop","title":"<code>stop()</code>","text":"<p>Stop the worker thread.</p> <p>Signals the worker thread to stop and waits for it to complete. Sends a sentinel value (None) to the input queue to wake up the worker if it's waiting.</p> Notes <p>This method blocks until the worker thread has fully stopped.</p> Source code in <code>src/async_task_pipeline/base/stage.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the worker thread.\n\n    Signals the worker thread to stop and waits for it to complete.\n    Sends a sentinel value (None) to the input queue to wake up the\n    worker if it's waiting.\n\n    Notes\n    -----\n    This method blocks until the worker thread has fully stopped.\n    \"\"\"\n    self.running = False\n    self.input_queue.put(None)\n    if self.thread:\n        self.thread.join()\n    logger.info(f\"Stopped pipeline stage: {self.name}\")\n</code></pre>"},{"location":"api/stage/#async_task_pipeline.base.stage.PipelineStage.get_average_processing_time","title":"<code>get_average_processing_time()</code>","text":"<p>Get average processing time for this stage.</p> <p>Calculates the average time spent in the processing function across all processed items.</p> <p>Returns:</p> Type Description <code>float</code> <p>Average processing time in seconds, or 0.0 if timing is disabled or no items have been processed.</p> Source code in <code>src/async_task_pipeline/base/stage.py</code> <pre><code>def get_average_processing_time(self) -&gt; float:\n    \"\"\"Get average processing time for this stage.\n\n    Calculates the average time spent in the processing function\n    across all processed items.\n\n    Returns\n    -------\n    float\n        Average processing time in seconds, or 0.0 if timing is disabled\n        or no items have been processed.\n    \"\"\"\n    if not self.enable_timing:\n        return 0.0\n\n    if self.processed_count &gt; 0:\n        return self.total_processing_time / self.processed_count\n    return 0.0\n</code></pre>"},{"location":"api/stage/#async_task_pipeline.base.stage.PipelineStage.clear_input_queue","title":"<code>clear_input_queue()</code>","text":"<p>Clear the input queue</p> Source code in <code>src/async_task_pipeline/base/stage.py</code> <pre><code>def clear_input_queue(self) -&gt; None:\n    \"\"\"Clear the input queue\"\"\"\n    while not self.input_queue.empty():\n        self.input_queue.get()\n</code></pre>"},{"location":"api/utils/","title":"Utilities","text":"<p>Utility modules for performance analysis, logging, and metrics collection.</p>"},{"location":"api/utils/#performance-analysis","title":"Performance Analysis","text":""},{"location":"api/utils/#async_task_pipeline.utils.analysis.log_pipeline_performance_analysis","title":"<code>async_task_pipeline.utils.analysis.log_pipeline_performance_analysis(pipeline)</code>","text":"<p>Log comprehensive performance analysis for an AsyncTaskPipeline.</p> <p>Analyzes pipeline performance and logs detailed metrics including overall efficiency, per-stage breakdowns, and individual item timing analysis. This function is useful for identifying bottlenecks and optimizing pipeline performance.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>AsyncTaskPipeline</code> <p>The pipeline instance to analyze. Must have timing enabled and have processed at least one item.</p> required Notes <p>This function logs analysis results using the pipeline's logger. If timing is disabled on the pipeline, only a warning message is logged.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pipeline = AsyncTaskPipeline(enable_timing=True)\n&gt;&gt;&gt; # ... process some data ...\n&gt;&gt;&gt; log_pipeline_performance_analysis(pipeline)\n</code></pre> Source code in <code>src/async_task_pipeline/utils/analysis.py</code> <pre><code>def log_pipeline_performance_analysis(pipeline: \"AsyncTaskPipeline\") -&gt; None:\n    \"\"\"Log comprehensive performance analysis for an AsyncTaskPipeline.\n\n    Analyzes pipeline performance and logs detailed metrics including overall\n    efficiency, per-stage breakdowns, and individual item timing analysis.\n    This function is useful for identifying bottlenecks and optimizing\n    pipeline performance.\n\n    Parameters\n    ----------\n    pipeline : AsyncTaskPipeline\n        The pipeline instance to analyze. Must have timing enabled and\n        have processed at least one item.\n\n    Notes\n    -----\n    This function logs analysis results using the pipeline's logger.\n    If timing is disabled on the pipeline, only a warning message is logged.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pipeline = AsyncTaskPipeline(enable_timing=True)\n    &gt;&gt;&gt; # ... process some data ...\n    &gt;&gt;&gt; log_pipeline_performance_analysis(pipeline)\n    \"\"\"\n    if not pipeline.enable_timing:\n        logger.info(\"Pipeline timing is disabled. No analysis available.\")\n        return\n\n    logger.info(\"Enhanced Pipeline Performance Analysis:\")\n    summary = pipeline.get_latency_summary()\n    logger.info(f\"Total items processed: {summary['total_items']}\")\n    logger.info(f\"Average end-to-end latency: {summary['avg_total_latency']:.3f}s\")\n\n    efficiency = summary[\"overall_efficiency\"]\n    logger.info(\"Overall Efficiency Metrics:\")\n    logger.info(f\"  Computation efficiency: {efficiency['computation_efficiency']:.1%}\")\n    logger.info(f\"  Overhead ratio: {efficiency['overhead_ratio']:.1%}\")\n\n    logger.info(\"Per-Stage Performance Breakdown:\")\n    for stage_name, stats in summary[\"stage_statistics\"].items():\n        timing = stats[\"timing_breakdown\"]\n        logger.info(f\"  {stage_name}:\")\n        logger.info(f\"    Processed: {stats['processed_count']} items\")\n        logger.info(f\"    Avg computation time: {timing['avg_computation_time'] * 1000:.2f}ms\")\n        logger.info(f\"    Avg queue wait time: {timing['avg_queue_wait_time'] * 1000:.2f}ms\")\n        logger.info(f\"    Avg transmission time: {timing['avg_transmission_time'] * 1000:.2f}ms\")\n        logger.info(f\"    Computation ratio: {timing['computation_ratio']:.1%}\")\n\n    logger.info(\"Detailed Analysis for First Few Items:\")\n    for item in pipeline.completed_items[:3]:\n        breakdown = item.get_timing_breakdown()\n        logger.info(f\" Item {item.seq_num}:\")\n\n        if breakdown is not None and \"totals\" in breakdown:\n            totals = breakdown[\"totals\"]\n            logger.info(f\"  Total latency: {totals['total_latency'] * 1000:.2f}ms\")\n            logger.info(f\"  Actual computation time: {totals['total_computation_time'] * 1000:.2f}ms\")\n            logger.info(f\"  Actual overhead time: {totals['total_overhead_time'] * 1000:.2f}ms\")\n            logger.info(f\"  Computation ratio: {totals['computation_ratio']:.1%}\")\n</code></pre>"},{"location":"api/utils/#metrics","title":"Metrics","text":""},{"location":"api/utils/#async_task_pipeline.utils.metrics.DetailedTiming","title":"<code>async_task_pipeline.utils.metrics.DetailedTiming</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Detailed timing information for a pipeline stage.</p> <p>Captures precise timing measurements for different phases of item processing within a pipeline stage, enabling detailed performance analysis and bottleneck identification.</p> <p>Parameters:</p> Name Type Description Default <code>queue_enter_time</code> <code>float</code> <p>Timestamp when the item entered the stage's input queue.</p> required <code>processing_start_time</code> <code>float</code> <p>Timestamp when the stage began processing the item.</p> required <code>processing_end_time</code> <code>float</code> <p>Timestamp when the stage finished processing the item.</p> required <code>queue_exit_time</code> <code>float</code> <p>Timestamp when the processed item was placed in the output queue.</p> required"},{"location":"api/utils/#async_task_pipeline.utils.metrics.DetailedTiming.queue_wait_time","title":"<code>queue_wait_time</code>  <code>property</code>","text":"<p>Time spent waiting in input queue.</p> <p>Returns:</p> Type Description <code>float</code> <p>Duration in seconds between queue entry and processing start.</p>"},{"location":"api/utils/#async_task_pipeline.utils.metrics.DetailedTiming.computation_time","title":"<code>computation_time</code>  <code>property</code>","text":"<p>Time spent in actual computation.</p> <p>Returns:</p> Type Description <code>float</code> <p>Duration in seconds of the actual processing function execution.</p>"},{"location":"api/utils/#async_task_pipeline.utils.metrics.DetailedTiming.transmission_time","title":"<code>transmission_time</code>  <code>property</code>","text":"<p>Time spent in transmission to next stage.</p> <p>Returns:</p> Type Description <code>float</code> <p>Duration in seconds between processing completion and output queue placement.</p>"},{"location":"api/utils/#logging","title":"Logging","text":""},{"location":"api/utils/#async_task_pipeline.utils.logging","title":"<code>async_task_pipeline.utils.logging</code>","text":"<p>Logging utilities for the async task pipeline framework.</p> <p>This module provides a centralized logger instance used throughout the pipeline framework for consistent logging behavior.</p>"},{"location":"api/utils/#async_task_pipeline.utils.logging.logger","title":"<code>logger = logging.getLogger('async_task_pipeline')</code>  <code>module-attribute</code>","text":"<p>Logger instance for the async task pipeline framework.</p> <p>This logger is used throughout the framework for consistent logging. Configure it at the application level to control log output format, level, and destinations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import logging\n&gt;&gt;&gt; from async_task_pipeline.utils import logger\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Configure logging level\n&gt;&gt;&gt; logger.setLevel(logging.INFO)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Add a handler\n&gt;&gt;&gt; handler = logging.StreamHandler()\n&gt;&gt;&gt; logger.addHandler(handler)\n</code></pre>"}]}
